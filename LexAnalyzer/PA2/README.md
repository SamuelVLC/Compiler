README file for Programming Assignment 2 (C++ edition)
=====================================================

Your directory should now contain the following files:

 Makefile        -> [course dir]/src/PA2/Makefile
 README
 cool.flex
 test.cl
 lextest.cc      -> [course dir]/src/PA2/lextest.cc
 mycoolc         -> [course dir]/src/PA2/mycoolc
 stringtab.cc    -> [course dir]/src/PA2/stringtab.cc
 utilities.cc    -> [course dir]/src/PA2/utilities.cc
 handle_flags.cc -> [course dir]/src/PA2/handle_flags.cc
 *.d             dependency files
 *.*             other generated files

The include (.h) files for this assignment can be found in 
[course dir]/include/PA2

	The Makefile contains targets for compiling and running your
	program. DO NOT MODIFY.

	The README contains this info. Part of the assignment is to fill
	the README with the write-up for your project. You should
	explain design decisions, explain why your code is correct, and
	why your test cases are adequate. It is part of the assignment
	to clearly and concisely explain things in text as well as to
	comment your code. Just edit this file.

	cool.flex is a skeleton file for the specification of the
	lexical analyzer. You should complete it with your regular
	expressions, patterns and actions. Information on how to do this
	is in the flex manual, which is part of your reader.

	test.cl is a COOL program that you can test the lexical
	analyzer on. It contains some errors, so it won't compile with
	coolc. However, test.cl does not exercise all lexical
	constructs of COOL and part of your assignment is to rewrite
	test.cl with a complete set of tests for your lexical analyzer.

	cool-parse.h contains definitions that are used by almost all parts
	of the compiler. DO NOT MODIFY.

	stringtab.{cc|h} and stringtab_functions.h contains functions
        to manipulate the string tables.  DO NOT MODIFY.

	utilities.{cc|h} contains functions used by the main() part of
	the lextest program. You may want to use the strdup() function
	defined in here. Remember that you should not print anything
	from inside cool.flex! DO NOT MODIFY.

	lextest.cc contains the main function which will call your
	lexer and print out the tokens that it returns.  DO NOT MODIFY.

	mycoolc is a shell script that glues together the phases of the
	compiler using Unix pipes instead of statically linking code.  
	While inefficient, this architecture makes it easy to mix and match
	the components you write with those of the course compiler.
	DO NOT MODIFY.	

        cool-lexer.cc is the scanner generated by flex from cool.flex.
        DO NOT MODIFY IT, as your changes will be overritten the next
        time you run flex.

 	The *.d files are automatically generated Makefiles that capture
 	dependencies between source and header files in this directory.
 	These files are updated automatically by Makefile; see the gmake
 	documentation for a detailed explanation.

Instructions
------------

	To compile your lextest program type:

	% gmake lexer

	Run your lexer by putting your test input in a file 'foo.cl' and
	run the lextest program:

	% ./lexer foo.cl

	To run your lexer on the file test.cl type:

	% gmake dotest

	If you think your lexical analyzer is correct and behaves like
	the one we wrote, you can actually try 'mycoolc' and see whether
	it runs and produces correct code for any examples.
	If your lexical analyzer behaves in an
	unexpected manner, you may get errors anywhere, i.e. during
	parsing, during semantic analysis, during code generation or
	only when you run the produced code on spim. So beware.

	To turnin your work type:

	% gmake submit-clean

	And run the "submit" program following the instructions on the
	course web page.
	
	Running "submit" will collect the files cool.flex, test.cl,
	README, and test.output. Don't forget to edit the README file to
	include your write-up, and to write your own test cases in
	test.cl.

 	You may turn in the assignment as many times as you like.
	However, only the last version will be retained for
	grading.

	If you change architectures you must issue

	% gmake clean

	when you switch from one type of machine to the other.
	If at some point you get weird errors from the linker,	
	you probably forgot this step.

	GOOD LUCK!

---8<------8<------8<------8<---cut here---8<------8<------8<------8<---

Write-up for PA2
----------------

### Testes Adicionais 

Adicionamos alguns testes que representam validações de um analisador léxico para linguagem COOL e outros pedidos compartilhados na descrição do TP2.

`./lexer test1.txt`
`./lexer test2.cl`

### Algumas das expressões regulares usadas para identificar diferentes tokens no analisador léxico:

1. **Palavras-chave (Keywords):**
   - As palavras-chave da linguagem Cool são definidas usando expressões regulares simples, como `{CLASS}`, `{ELSE}`, `{IF}`, `{FI}`, `{IN}`, `{INHERITS}`, `{ISVOID}`, `{LET}`, `{LOOP}`, `{POOL}`, `{THEN}`, `{WHILE}`, `{CASE}`, `{ESAC}`, `{NEW}`, `{OF}`, `{NOT}`, `{BOOL_CONST}`. 
   - Cada uma dessas expressões corresponde a uma palavra-chave específica da linguagem Cool. Por exemplo, `{CLASS}` corresponde à palavra-chave "class".

2. **Constantes Inteiras (INT_CONST):**
   - A constante inteira é identificada pela expressão regular `{INT_CONST}`, que corresponde a sequências de dígitos de 0 a 9.

3. **Identificadores de Objetos (OBJECTID) e Identificadores de Tipos (TYPEID):**
   - Os identificadores de objetos e tipos são identificados pelas expressões regulares `{OBJECTID}` e `{TYPEID}`, respectivamente.
   - Os identificadores de objetos começam com uma letra minúscula seguida de letras maiúsculas, minúsculas, dígitos ou sublinhados.
   - Os identificadores de tipos começam com uma letra maiúscula seguida das mesmas possibilidades que os identificadores de objetos.

4. **Constantes Booleanas (BOOL_CONST):**
   - As constantes booleanas "true" e "false" são identificadas pela expressão regular `{BOOL_CONST}`.

5. **Operadores e Símbolos Especiais:**
   - Operadores e símbolos especiais são identificados diretamente por seus caracteres correspondentes. Por exemplo, `">="`, `"<="`, `"<-"`, `"+"`, `"/"`, `"-"`, `"*"`, `"="`, `"<"`, `">"`, `"."`, `";"`, `":"`, `"("`, `")"`, `"@"`, `"{"`, `"}"`, `","`, `"~"`.
   - Esses tokens são identificados diretamente pelas sequências de caracteres correspondentes.

6. **Constantes de String (STR_CONST):**
   - As constantes de string são identificadas pela expressão regular `"\"`. Isso marca o início de uma constante de string.
   - A constante de string é encerrada quando outra aspa dupla (`"`) é encontrada, após possíveis caracteres de escape, como `"\n"` e `"\0"`.


---
# Tratamento de String

1. **Armazenamento de Lexemas em uma Tabela de Strings:**
   - [x] Implementação da tabela de strings, Inclusão de todos os lexemas na tabela, Utilização eficiente de espaço e tempo para armazenamento.

2. **Tratamento de Identificadores Especiais:**
   - [x] Tratamento uniforme de identificadores especiais (Object, Int, Bool, String, SELF_TYPE e self), com tratamento igualitário de identificadores especiais e regulares nesta fase do compilador.

3. **Manipulação Simplificada de Literais Inteiros:**
   - [x] Tratamento dos literais inteiros como tokens de identificadores.
   - [x] Armazenamento de literais inteiros na tabela de strings sem verificar o comprimento.

4. **Processamento de Constantes de String:**
   - [x] Conversão de caracteres de escape em constantes de string para seus valores correspondentes.
   - [x] Reporte de erros para strings contendo o caractere nulo.
   - [x] Permissão da sequência `"\0"`, convertendo-a para o caractere nulo.


---
# Tratamento de Erros

   - [ ] O analisador léxico não imprime nada e comunica erros ao parser retornando uma token especial de erro chamada `ERROR`.
   - [ ] Quando um caractere inválido é encontrado, uma string contendo apenas esse caractere é retornada como string de erro.
   - [ ] Se um string contiver uma nova linha sem caractere de escape, é relatado como "Unterminated string constant" e a análise léxica continua no início da próxima linha.
   - [x] Quando um string é muito longo, é relatado como "String constant too long" no string de erro do token `ERROR`. Se o string contiver caracteres inválidos, é relatado como "String contains null character".
   - [ ] Se um comentário permanecer aberto quando o EOF for encontrado, é relatado como "EOF in comment". Não se tokenizar o conteúdo do comentário, mesmo se o terminador estiver faltando.
   - [ ] Se um EOF for encontrado antes do fechamento de um string, é relatado como "EOF in string constant".
   - [x] Se "*)" for encontrado fora de um comentário, é relatado como "Unmatched *)".
   - [ ] A análise léxica continua após os erros, de acordo com as regras especificadas para cada tipo de erro. Por exemplo, no caso de um string muito longo, a análise continua após o final do string.
   - [ ] Todos os erros são retornados como tokens especiais `ERROR`, conforme exigido, em vez de serem impressos diretamente.
   - [ ] A token `error` (com letra minúscula) é ignorada para essa tarefa, já que será usada pelo parser no TP03.


